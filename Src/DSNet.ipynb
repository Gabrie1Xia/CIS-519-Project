{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26MhA1POPkw0"
   },
   "source": [
    "# DSNet:  Automatic Dermoscopic Skin Lesion Segmentation\n",
    "\n",
    "\n",
    "* Through  this  study,  we  present  a  new  and  automatic  semantic  segmentation  network for robust skin lesion segmentation named Dermoscopic Skin Network (DSNet).\n",
    "\n",
    "* In order to reduce the number of parameters to make the network lightweight, we used a depth-wise separable convolution in lieu of standard convolution to project the learnt discriminating features onto the pixel space at different stages of the encoder.\n",
    "\n",
    "* For any query: \n",
    "        ** Md. Kamrul Hasan \n",
    "        ** M.Sc. in Medical Imaging and Applications (MAIA)\n",
    "        ** Erasmus Scholar [2017-2019] \n",
    "        ** Contact: kamruleeekuet@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pv8J8rFPPkw2"
   },
   "source": [
    "## Import all the python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ot2ezA6Pkw3",
    "outputId": "8d723d06-3d01-40c5-8b00-4a92c88d47a2"
   },
   "outputs": [],
   "source": [
    "#------------------------------Keras Packages----------------------------------- \n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.losses import *\n",
    "from keras import backend as keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.densenet import DenseNet201, DenseNet121\n",
    "#from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model#\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------Others python Packages---------------------------\n",
    "import numpy as np # High-level mathematical functions for n-dimensional arrays \n",
    "import os   \n",
    "import cv2\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from keras.initializers import Constant\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.morphology import disk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qvwfz5aXPkw8"
   },
   "source": [
    "## Designing of Proposed DSNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvQai2xaPkw9"
   },
   "outputs": [],
   "source": [
    "def DSNet(nClasses, input_height, input_width):\n",
    "\n",
    "    #------------------------------Define Input Shape----------------------------------\n",
    "    \n",
    "    img_input = Input(shape=(input_height, input_width, 3)) # defining the Input shape \n",
    "    \n",
    "    #Load DenseNet121 from keras. This model is initialized with the ImageNet. \n",
    "    #This part is responsible for the feature extraction which is so called convolution\n",
    "    # part of the semantic segmentation (Encoder). \n",
    "    \n",
    "    Encoder_Dense = DenseNet121( weights = 'imagenet',\n",
    "\t\t\t\t\t\tinclude_top = False,\n",
    "\t\t\t\t\t\tinput_tensor = img_input) \n",
    "\n",
    "    Encoder = SeparableConv2D(filters = 1024,\n",
    "\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tpadding=\"same\")(Encoder_Dense.output)\n",
    "    Encoder = BatchNormalization()(Encoder)\n",
    "\n",
    "    \n",
    "    # Decoding the encoded features to reconstruct the original input shape Image.\n",
    "\n",
    "    Decoder = UpSampling2D(size = (2, 2))(Encoder)\n",
    "    Decoder = concatenate([Encoder_Dense.get_layer(name=\"pool3_pool\").output, Decoder], axis=-1)\n",
    "    Decoder = SeparableConv2D(filters = 1024,\n",
    "\t\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "    \n",
    "    Decoder = UpSampling2D(size = (2, 2))(Decoder)\n",
    "    Decoder = concatenate([Encoder_Dense.get_layer(name=\"pool2_pool\").output, Decoder], axis=-1)\n",
    "    Decoder = SeparableConv2D(filters = 512,\n",
    "\t\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "\n",
    "    Decoder = UpSampling2D(size = (2, 2))(Decoder)\n",
    "    Decoder = concatenate([Encoder_Dense.get_layer(name=\"pool1\").output, Decoder], axis=-1)\n",
    "    Decoder = SeparableConv2D(filters = 256,\n",
    "\t\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "\n",
    "    Decoder = UpSampling2D( size = (2, 2))(Decoder)\n",
    "    Decoder = concatenate([Encoder_Dense.get_layer(name=\"conv1/bn\").output, Decoder], axis=-1)\n",
    "    Decoder = SeparableConv2D(filters = 128,\n",
    "\t\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "\n",
    "    Decoder = UpSampling2D(size = (2, 2))(Decoder)    \n",
    "    Decoder = SeparableConv2D(filters = 64,\n",
    "\t\t\t\t\t\tkernel_size = (3, 3),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "\n",
    "    Decoder = SeparableConv2D(filters = nClasses,\n",
    "\t\t\t\t\t\tkernel_size = (1, 1),\n",
    "\t\t\t\t\t\tactivation = 'relu',\n",
    "\t\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\t\tpadding = \"same\")(Decoder)\n",
    "    Decoder = BatchNormalization()(Decoder)\n",
    "    \n",
    "\n",
    "    Predicted_Mask = Conv2D(filters = 1,\n",
    "\t\t\t\t\t\t\t\t\tkernel_size = 1,\n",
    "\t\t\t\t\t\t\t\t\tactivation = 'sigmoid')(Decoder)\n",
    "    \n",
    "    \n",
    "    DSNet_model = Model(img_input,Predicted_Mask)###\n",
    "\n",
    "    return DSNet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4tHE2mgPkxA"
   },
   "source": [
    "## Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIPFj2btPkxB"
   },
   "outputs": [],
   "source": [
    "def DataPreProcessing(img, mask):\n",
    "    img = preprocess_input(img) # Data standardization meaning 0 mean and unit variance.\n",
    "    img = img/img.max()         # Data Normalization\n",
    "    mask = mask/mask.max()      # Mask Normalization\n",
    "    return (img, mask)          # Return Tuple of Original image along with GT image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_xr-b9UPkxE"
   },
   "source": [
    "## Function for Train generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojIqKID9PkxE"
   },
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,\n",
    "                   train_path,\n",
    "                   image_folder,\n",
    "                   mask_folder,\n",
    "                   aug_dict,\n",
    "                   image_color_mode = \"rgb\",\n",
    "                   mask_color_mode = \"grayscale\",\n",
    "                   image_save_prefix  = \"image\",\n",
    "                   mask_save_prefix  = \"mask\",\n",
    "                   save_to_dir = None,\n",
    "                   target_size = (192,256),\n",
    "                   seed = 1):\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = 'rgb',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "#         save_to_dir = save_to_dir,\n",
    "#         save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "#         save_to_dir = save_to_dir,\n",
    "#         save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = DataPreProcessing(img,mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2WcrkluPkxH"
   },
   "source": [
    "## Function for Test and Validation generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ji2yxioePkxI"
   },
   "outputs": [],
   "source": [
    "def ValGenerator(batch_size,\n",
    "                 val_path,\n",
    "                 image_folder,\n",
    "                 mask_folder,\n",
    "                 target_size = (192,256),\n",
    "                 seed = 1):\n",
    "    \n",
    "    image_datagen = ImageDataGenerator()\n",
    "    mask_datagen = ImageDataGenerator()\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = 'rgb',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "\n",
    "    \n",
    "    val_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img,mask) in val_generator:\n",
    "        img,mask = DataPreProcessing(img,mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ua6ca01PkxL"
   },
   "source": [
    "## Function for Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMEHH8qfPkxM"
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def Jaccard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection ) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection)\n",
    "\n",
    "def Jaccard_coef_loss(y_true, y_pred):\n",
    "    return (1-Jaccard_coef(y_true, y_pred))\n",
    "\n",
    "def bcc_Jaccard_coef_loss(y_true, y_pred):\n",
    "    return (binary_crossentropy(y_true, y_pred)+(1-Jaccard_coef(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_h2WpqUPkxP"
   },
   "source": [
    "## Train and Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwpgyM8UPkxQ",
    "outputId": "c93afb9a-8ab7-4c45-ae02-f00448d52e88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadding2  (None, 262, 262, 3)  0          ['input_3[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 128, 128, 64  9408        ['zero_padding2d_4[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 128, 128, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_5 (ZeroPadding2  (None, 130, 130, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 64, 64, 64)   0           ['zero_padding2d_5[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 64, 64, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 64, 64, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 64, 64, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 64, 64, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 64, 64, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 64, 64, 160)  640        ['conv2_block3_concat[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 64, 64, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 64, 64, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 64, 64, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 64, 64, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 64, 64, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 64, 64, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 64, 64, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 64, 64, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 64, 64, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 64, 64, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 64, 64, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 64, 64, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 64, 64, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 64, 64, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 64, 64, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 64, 64, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 64, 64, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 32, 32, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 32, 32, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv3_block2_0_bn (BatchNormal  (None, 32, 32, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 32, 32, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 32, 32, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 32, 32, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 32, 32, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 32, 32, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 32, 32, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 32, 32, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 32, 32, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 32, 32, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 32, 32, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 32, 32, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 32, 32, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 32, 32, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 32, 32, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 32, 32, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 32, 32, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 32, 32, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 32, 32, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 32, 32, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 32, 32, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 32, 32, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 32, 32, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 32, 32, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 32, 32, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 32, 32, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 32, 32, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 32, 32, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 32, 32, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 32, 32, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 32, 32, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 32, 32, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv3_block10_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 32, 32, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 32, 32, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 32, 32, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 32, 32, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 32, 32, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 32, 32, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 32, 32, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 32, 32, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 32, 32, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 32, 32, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 32, 32, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 32, 32, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 32, 32, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 32, 32, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 32, 32, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 16, 16, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 16, 16, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 16, 16, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 16, 16, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 16, 16, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 16, 16, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 16, 16, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 16, 16, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 16, 16, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 16, 16, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 16, 16, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 16, 16, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 16, 16, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 16, 16, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 16, 16, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 16, 16, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 16, 16, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 16, 16, 448)  1792       ['conv4_block6_concat[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 16, 16, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 16, 16, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 16, 16, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 16, 16, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 16, 16, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 16, 16, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 16, 16, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 16, 16, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 16, 16, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 16, 16, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 16, 16, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 16, 16, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 16, 16, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 16, 16, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 16, 16, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 16, 16, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 16, 16, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 16, 16, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block11_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 16, 16, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 16, 16, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 16, 16, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 16, 16, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 16, 16, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 16, 16, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 16, 16, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 16, 16, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 16, 16, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 16, 16, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 16, 16, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 16, 16, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 16, 16, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 16, 16, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 16, 16, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 16, 16, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 16, 16, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 16, 16, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 16, 16, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 16, 16, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 16, 16, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 16, 16, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 16, 16, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 16, 16, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 16, 16, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 16, 16, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 16, 16, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 16, 16, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 16, 16, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 16, 16, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 16, 16, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 16, 16, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 16, 16, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 16, 16, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block20_0_relu (Activati  (None, 16, 16, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 16, 16, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 16, 16, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 16, 16, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 16, 16, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 16, 16, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 16, 16, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 16, 16, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 16, 16, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 16, 16, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 16, 16, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 16, 16, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 16, 16, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 16, 16, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 16, 16, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 16, 16, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 16, 16, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 16, 16, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 16, 16, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 16, 16, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 16, 16, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 16, 16, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 16, 16, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 16, 16, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 16, 16, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 8, 8, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 8, 8, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 8, 8, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 8, 8, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 8, 8, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 8, 8, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 8, 8, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 8, 8, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 8, 8, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 8, 8, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 8, 8, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 8, 8, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 8, 8, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 8, 8, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 8, 8, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 8, 8, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 8, 8, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 8, 8, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 8, 8, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 8, 8, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 8, 8, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 8, 8, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 8, 8, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 8, 8, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 8, 8, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 8, 8, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 8, 8, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv5_block8_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 8, 8, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 8, 8, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 8, 8, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 8, 8, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 8, 8, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 8, 8, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 8, 8, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 8, 8, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 8, 8, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 8, 8, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 8, 8, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 8, 8, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 8, 8, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 8, 8, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 8, 8, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 8, 8, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 8, 8, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 8, 8, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 8, 8, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 8, 8, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 8, 8, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 8, 8, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 8, 8, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 8, 8, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 8, 8, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 8, 8, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 8, 8, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 8, 8, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 8, 8, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 8, 8, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 8, 8, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 8, 8, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 8, 8, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 8, 8, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 8, 8, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 8, 8, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 8, 8, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 8, 8, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 8, 8, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 8, 8, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 8, 8, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 8, 8, 1024)  1058816     ['relu[0][0]']                   \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 1024)  4096        ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampling2D  (None, 16, 16, 1024  0          ['batch_normalization_14[0][0]'] \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 16, 16, 1280  0           ['pool3_pool[0][0]',             \n",
      "                                )                                 'up_sampling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 16, 16, 1024  1323264    ['concatenate_8[0][0]']          \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 1024  4096       ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampling2D  (None, 32, 32, 1024  0          ['batch_normalization_15[0][0]'] \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 1152  0           ['pool2_pool[0][0]',             \n",
      "                                )                                 'up_sampling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 32, 32, 512)  600704     ['concatenate_9[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 512)  2048       ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_12 (UpSampling2D  (None, 64, 64, 512)  0          ['batch_normalization_16[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 64, 64, 576)  0           ['pool1[0][0]',                  \n",
      "                                                                  'up_sampling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 64, 64, 256)  152896     ['concatenate_10[0][0]']         \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_13 (UpSampling2D  (None, 128, 128, 25  0          ['batch_normalization_17[0][0]'] \n",
      " )                              6)                                                                \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 128, 128, 32  0           ['conv1/bn[0][0]',               \n",
      "                                0)                                'up_sampling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " separable_conv2d_18 (Separable  (None, 128, 128, 12  43968      ['concatenate_11[0][0]']         \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_18[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_14 (UpSampling2D  (None, 256, 256, 12  0          ['batch_normalization_18[0][0]'] \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_19 (Separable  (None, 256, 256, 64  9408       ['up_sampling2d_14[0][0]']       \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_19[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_20 (Separable  (None, 256, 256, 2)  194        ['batch_normalization_19[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 256, 256, 2)  8          ['separable_conv2d_20[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 256, 256, 1)  3           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,238,797\n",
      "Trainable params: 10,149,129\n",
      "Non-trainable params: 89,668\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This is for the list of the augmentation (Geometric Augmentation). This \n",
    "# geometric augmentation will increase the numbers of training images to\n",
    "# overcome over-fiting and curse of dimensionality. This will act as a online \n",
    "# aumentation that will help to reduce the memory consumtion during the \n",
    "# Trainig phase. If you want you can augment the images before and save it to\n",
    "# the directory and read them.\n",
    "\n",
    "data_gen_args = dict(rotation_range=90,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='wrap')\n",
    "\n",
    "CurrentDirectory=os.getcwd()\n",
    "\n",
    "height=256#192\n",
    "width=256\n",
    "\n",
    "# Calling the train generator function with required arguments. \n",
    "TrainGen= trainGenerator(batch_size=12,\n",
    "                         train_path=CurrentDirectory+'/isic-challenge-2016_split/',\n",
    "                         image_folder='Input',\n",
    "                         mask_folder='Mask',\n",
    "                         aug_dict=data_gen_args,\n",
    "                         image_color_mode = \"grayscale\",\n",
    "                         mask_color_mode = \"grayscale\",\n",
    "                         image_save_prefix  = \"image\",\n",
    "                         mask_save_prefix  = \"mask\",\n",
    "#                          save_to_dir = CurrentDirectory+'/aug/',\n",
    "                         target_size = (height,width),\n",
    "                         seed = 1)\n",
    " \n",
    "# Calling the Validation/ Test generator function with required arguments. \n",
    "TestGen= ValGenerator(batch_size=12,\n",
    "                      val_path=CurrentDirectory+'/isic-challenge-2016_split/',\n",
    "                      image_folder='Val_Input', \n",
    "                      mask_folder='Val_Mask',\n",
    "                      target_size = (height,width),\n",
    "                      seed = 1)\n",
    "\n",
    "\n",
    "# Calling the designed Model to train.\n",
    "\n",
    "model = DSNet(2, height, width)\n",
    "\n",
    "model.compile(optimizer = 'adadelta',\n",
    "                loss = bcc_Jaccard_coef_loss,\n",
    "                metrics = [Jaccard_coef])\n",
    "\n",
    "# This will plot a graph of the model and save it to a file. \n",
    "plot_model(model, show_shapes=True, to_file='model.png')\n",
    "\n",
    "# This will print the model summary and total parameters for each operations\n",
    "# along with total learnable parameters.\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Save the model after every epoch. You can save only the best model. If the \n",
    "# specified metric do not improve it will not save the that current epoch. \n",
    "\n",
    "model_checkpoint = ModelCheckpoint('Trained_Model.hdf5',\n",
    "                                   monitor='val_Jaccard_coef',\n",
    "                                   verbose=1,\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_Jaccard_coef',\n",
    "                              factor=0.5,\n",
    "                              patience=8,\n",
    "                              verbose=1,\n",
    "                              mode='max',\n",
    "                              min_lr=0.00000001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_Jaccard_coef',\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Trains the model on data generated batch-by-batch by a Python generator \n",
    "# (or an instance of Sequence). The generator is run in parallel to the model,\n",
    "# for efficiency. For instance, this allows you to do real-time data\n",
    "# augmentation on images on CPU in parallel to training your model on GPU.\n",
    "\n",
    "\n",
    "# history=model.fit_generator(TrainGen, \n",
    "#                             steps_per_epoch=400, \n",
    "#                             epochs=200, \n",
    "#                             verbose=1, \n",
    "#                             validation_data= TestGen, \n",
    "#                             validation_steps=30, \n",
    "#                             callbacks=[model_checkpoint, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yiang gong\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 1 classes.\n",
      "Found 540 images belonging to 1 classes.\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.7019 - Jaccard_coef: 0.2034Found 180 images belonging to 1 classes.\n",
      "Found 180 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: val_Jaccard_coef improved from -inf to 0.23789, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 51s 965ms/step - loss: 1.7019 - Jaccard_coef: 0.2034 - val_loss: 1.4587 - val_Jaccard_coef: 0.2379 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6439 - Jaccard_coef: 0.2172\n",
      "Epoch 2: val_Jaccard_coef did not improve from 0.23789\n",
      "45/45 [==============================] - 41s 915ms/step - loss: 1.6439 - Jaccard_coef: 0.2172 - val_loss: 1.4642 - val_Jaccard_coef: 0.2365 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6021 - Jaccard_coef: 0.2285\n",
      "Epoch 3: val_Jaccard_coef improved from 0.23789 to 0.23857, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.6021 - Jaccard_coef: 0.2285 - val_loss: 1.4671 - val_Jaccard_coef: 0.2386 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.5554 - Jaccard_coef: 0.2425\n",
      "Epoch 4: val_Jaccard_coef did not improve from 0.23857\n",
      "45/45 [==============================] - 44s 980ms/step - loss: 1.5554 - Jaccard_coef: 0.2425 - val_loss: 1.4745 - val_Jaccard_coef: 0.2380 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.5328 - Jaccard_coef: 0.2482\n",
      "Epoch 5: val_Jaccard_coef improved from 0.23857 to 0.24544, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.5328 - Jaccard_coef: 0.2482 - val_loss: 1.4751 - val_Jaccard_coef: 0.2454 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4847 - Jaccard_coef: 0.2638\n",
      "Epoch 6: val_Jaccard_coef improved from 0.24544 to 0.25275, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 47s 1s/step - loss: 1.4847 - Jaccard_coef: 0.2638 - val_loss: 1.4783 - val_Jaccard_coef: 0.2528 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4646 - Jaccard_coef: 0.2713\n",
      "Epoch 7: val_Jaccard_coef did not improve from 0.25275\n",
      "45/45 [==============================] - 44s 979ms/step - loss: 1.4646 - Jaccard_coef: 0.2713 - val_loss: 1.5074 - val_Jaccard_coef: 0.2437 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4420 - Jaccard_coef: 0.2770\n",
      "Epoch 8: val_Jaccard_coef improved from 0.25275 to 0.25641, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.4420 - Jaccard_coef: 0.2770 - val_loss: 1.5085 - val_Jaccard_coef: 0.2564 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4146 - Jaccard_coef: 0.2873\n",
      "Epoch 9: val_Jaccard_coef did not improve from 0.25641\n",
      "45/45 [==============================] - 44s 981ms/step - loss: 1.4146 - Jaccard_coef: 0.2873 - val_loss: 1.5328 - val_Jaccard_coef: 0.2523 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3894 - Jaccard_coef: 0.2950\n",
      "Epoch 10: val_Jaccard_coef improved from 0.25641 to 0.26060, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 47s 1s/step - loss: 1.3894 - Jaccard_coef: 0.2950 - val_loss: 1.5291 - val_Jaccard_coef: 0.2606 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3629 - Jaccard_coef: 0.3055\n",
      "Epoch 11: val_Jaccard_coef improved from 0.26060 to 0.27184, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 956ms/step - loss: 1.3629 - Jaccard_coef: 0.3055 - val_loss: 1.5043 - val_Jaccard_coef: 0.2718 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3536 - Jaccard_coef: 0.3077\n",
      "Epoch 12: val_Jaccard_coef did not improve from 0.27184\n",
      "45/45 [==============================] - 42s 940ms/step - loss: 1.3536 - Jaccard_coef: 0.3077 - val_loss: 1.4806 - val_Jaccard_coef: 0.2645 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3337 - Jaccard_coef: 0.3146\n",
      "Epoch 13: val_Jaccard_coef improved from 0.27184 to 0.28579, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 945ms/step - loss: 1.3337 - Jaccard_coef: 0.3146 - val_loss: 1.3915 - val_Jaccard_coef: 0.2858 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3114 - Jaccard_coef: 0.3224\n",
      "Epoch 14: val_Jaccard_coef improved from 0.28579 to 0.30376, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 949ms/step - loss: 1.3114 - Jaccard_coef: 0.3224 - val_loss: 1.3057 - val_Jaccard_coef: 0.3038 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2902 - Jaccard_coef: 0.3296\n",
      "Epoch 15: val_Jaccard_coef improved from 0.30376 to 0.33386, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.2902 - Jaccard_coef: 0.3296 - val_loss: 1.2326 - val_Jaccard_coef: 0.3339 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2734 - Jaccard_coef: 0.3366\n",
      "Epoch 16: val_Jaccard_coef improved from 0.33386 to 0.35980, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.2734 - Jaccard_coef: 0.3366 - val_loss: 1.1888 - val_Jaccard_coef: 0.3598 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2517 - Jaccard_coef: 0.3431\n",
      "Epoch 17: val_Jaccard_coef improved from 0.35980 to 0.37073, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.2517 - Jaccard_coef: 0.3431 - val_loss: 1.1780 - val_Jaccard_coef: 0.3707 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2540 - Jaccard_coef: 0.3424\n",
      "Epoch 18: val_Jaccard_coef improved from 0.37073 to 0.38621, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.2540 - Jaccard_coef: 0.3424 - val_loss: 1.1647 - val_Jaccard_coef: 0.3862 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2371 - Jaccard_coef: 0.3492\n",
      "Epoch 19: val_Jaccard_coef improved from 0.38621 to 0.39892, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.2371 - Jaccard_coef: 0.3492 - val_loss: 1.1506 - val_Jaccard_coef: 0.3989 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2287 - Jaccard_coef: 0.3522\n",
      "Epoch 20: val_Jaccard_coef did not improve from 0.39892\n",
      "45/45 [==============================] - 45s 1000ms/step - loss: 1.2287 - Jaccard_coef: 0.3522 - val_loss: 1.1648 - val_Jaccard_coef: 0.3950 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2114 - Jaccard_coef: 0.3592\n",
      "Epoch 21: val_Jaccard_coef improved from 0.39892 to 0.40236, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.2114 - Jaccard_coef: 0.3592 - val_loss: 1.1637 - val_Jaccard_coef: 0.4024 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1993 - Jaccard_coef: 0.3628\n",
      "Epoch 22: val_Jaccard_coef did not improve from 0.40236\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.1993 - Jaccard_coef: 0.3628 - val_loss: 1.1593 - val_Jaccard_coef: 0.4004 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1897 - Jaccard_coef: 0.3673\n",
      "Epoch 23: val_Jaccard_coef improved from 0.40236 to 0.40964, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1897 - Jaccard_coef: 0.3673 - val_loss: 1.1451 - val_Jaccard_coef: 0.4096 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1868 - Jaccard_coef: 0.3671\n",
      "Epoch 24: val_Jaccard_coef did not improve from 0.40964\n",
      "45/45 [==============================] - 45s 996ms/step - loss: 1.1868 - Jaccard_coef: 0.3671 - val_loss: 1.1529 - val_Jaccard_coef: 0.4036 - lr: 0.0010\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - ETA: 0s - loss: 1.1759 - Jaccard_coef: 0.3712\n",
      "Epoch 25: val_Jaccard_coef improved from 0.40964 to 0.41585, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1759 - Jaccard_coef: 0.3712 - val_loss: 1.1244 - val_Jaccard_coef: 0.4159 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1630 - Jaccard_coef: 0.3762\n",
      "Epoch 26: val_Jaccard_coef improved from 0.41585 to 0.41996, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1630 - Jaccard_coef: 0.3762 - val_loss: 1.1269 - val_Jaccard_coef: 0.4200 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1474 - Jaccard_coef: 0.3829\n",
      "Epoch 27: val_Jaccard_coef improved from 0.41996 to 0.42178, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1474 - Jaccard_coef: 0.3829 - val_loss: 1.1161 - val_Jaccard_coef: 0.4218 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1456 - Jaccard_coef: 0.3827\n",
      "Epoch 28: val_Jaccard_coef improved from 0.42178 to 0.42493, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1456 - Jaccard_coef: 0.3827 - val_loss: 1.1085 - val_Jaccard_coef: 0.4249 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1340 - Jaccard_coef: 0.3869\n",
      "Epoch 29: val_Jaccard_coef did not improve from 0.42493\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.1340 - Jaccard_coef: 0.3869 - val_loss: 1.1066 - val_Jaccard_coef: 0.4186 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1320 - Jaccard_coef: 0.3878\n",
      "Epoch 30: val_Jaccard_coef improved from 0.42493 to 0.42558, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1320 - Jaccard_coef: 0.3878 - val_loss: 1.0993 - val_Jaccard_coef: 0.4256 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1169 - Jaccard_coef: 0.3933\n",
      "Epoch 31: val_Jaccard_coef improved from 0.42558 to 0.42667, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.1169 - Jaccard_coef: 0.3933 - val_loss: 1.0946 - val_Jaccard_coef: 0.4267 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1020 - Jaccard_coef: 0.4000\n",
      "Epoch 32: val_Jaccard_coef improved from 0.42667 to 0.43199, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.1020 - Jaccard_coef: 0.4000 - val_loss: 1.0839 - val_Jaccard_coef: 0.4320 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0923 - Jaccard_coef: 0.4036\n",
      "Epoch 33: val_Jaccard_coef improved from 0.43199 to 0.43232, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 961ms/step - loss: 1.0923 - Jaccard_coef: 0.4036 - val_loss: 1.0836 - val_Jaccard_coef: 0.4323 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0968 - Jaccard_coef: 0.4010\n",
      "Epoch 34: val_Jaccard_coef improved from 0.43232 to 0.43632, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 949ms/step - loss: 1.0968 - Jaccard_coef: 0.4010 - val_loss: 1.0728 - val_Jaccard_coef: 0.4363 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0854 - Jaccard_coef: 0.4059\n",
      "Epoch 35: val_Jaccard_coef did not improve from 0.43632\n",
      "45/45 [==============================] - 42s 938ms/step - loss: 1.0854 - Jaccard_coef: 0.4059 - val_loss: 1.0704 - val_Jaccard_coef: 0.4363 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0771 - Jaccard_coef: 0.4091\n",
      "Epoch 36: val_Jaccard_coef did not improve from 0.43632\n",
      "45/45 [==============================] - 42s 940ms/step - loss: 1.0771 - Jaccard_coef: 0.4091 - val_loss: 1.0785 - val_Jaccard_coef: 0.4267 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0711 - Jaccard_coef: 0.4116\n",
      "Epoch 37: val_Jaccard_coef improved from 0.43632 to 0.44598, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 45s 1000ms/step - loss: 1.0711 - Jaccard_coef: 0.4116 - val_loss: 1.0520 - val_Jaccard_coef: 0.4460 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0606 - Jaccard_coef: 0.4165\n",
      "Epoch 38: val_Jaccard_coef improved from 0.44598 to 0.45366, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 960ms/step - loss: 1.0606 - Jaccard_coef: 0.4165 - val_loss: 1.0337 - val_Jaccard_coef: 0.4537 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0512 - Jaccard_coef: 0.4195\n",
      "Epoch 39: val_Jaccard_coef did not improve from 0.45366\n",
      "45/45 [==============================] - 43s 967ms/step - loss: 1.0512 - Jaccard_coef: 0.4195 - val_loss: 1.0550 - val_Jaccard_coef: 0.4390 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0376 - Jaccard_coef: 0.4256\n",
      "Epoch 40: val_Jaccard_coef did not improve from 0.45366\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.0376 - Jaccard_coef: 0.4256 - val_loss: 1.0375 - val_Jaccard_coef: 0.4466 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0373 - Jaccard_coef: 0.4259\n",
      "Epoch 41: val_Jaccard_coef improved from 0.45366 to 0.45740, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.0373 - Jaccard_coef: 0.4259 - val_loss: 1.0234 - val_Jaccard_coef: 0.4574 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0413 - Jaccard_coef: 0.4229\n",
      "Epoch 42: val_Jaccard_coef did not improve from 0.45740\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.0413 - Jaccard_coef: 0.4229 - val_loss: 1.0375 - val_Jaccard_coef: 0.4428 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0285 - Jaccard_coef: 0.4283\n",
      "Epoch 43: val_Jaccard_coef improved from 0.45740 to 0.46156, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.0285 - Jaccard_coef: 0.4283 - val_loss: 1.0069 - val_Jaccard_coef: 0.4616 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0237 - Jaccard_coef: 0.4298\n",
      "Epoch 44: val_Jaccard_coef did not improve from 0.46156\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.0237 - Jaccard_coef: 0.4298 - val_loss: 1.0142 - val_Jaccard_coef: 0.4557 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0194 - Jaccard_coef: 0.4315\n",
      "Epoch 45: val_Jaccard_coef improved from 0.46156 to 0.46320, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 1.0194 - Jaccard_coef: 0.4315 - val_loss: 1.0045 - val_Jaccard_coef: 0.4632 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0072 - Jaccard_coef: 0.4369\n",
      "Epoch 46: val_Jaccard_coef did not improve from 0.46320\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.0072 - Jaccard_coef: 0.4369 - val_loss: 1.0041 - val_Jaccard_coef: 0.4616 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0050 - Jaccard_coef: 0.4382\n",
      "Epoch 47: val_Jaccard_coef did not improve from 0.46320\n",
      "45/45 [==============================] - 45s 1s/step - loss: 1.0050 - Jaccard_coef: 0.4382 - val_loss: 1.0026 - val_Jaccard_coef: 0.4621 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9919 - Jaccard_coef: 0.4440\n",
      "Epoch 48: val_Jaccard_coef improved from 0.46320 to 0.46397, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9919 - Jaccard_coef: 0.4440 - val_loss: 0.9908 - val_Jaccard_coef: 0.4640 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9921 - Jaccard_coef: 0.4428\n",
      "Epoch 49: val_Jaccard_coef improved from 0.46397 to 0.47234, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9921 - Jaccard_coef: 0.4428 - val_loss: 0.9840 - val_Jaccard_coef: 0.4723 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9761 - Jaccard_coef: 0.4500\n",
      "Epoch 50: val_Jaccard_coef did not improve from 0.47234\n",
      "45/45 [==============================] - 45s 1000ms/step - loss: 0.9761 - Jaccard_coef: 0.4500 - val_loss: 0.9759 - val_Jaccard_coef: 0.4718 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9730 - Jaccard_coef: 0.4507\n",
      "Epoch 51: val_Jaccard_coef did not improve from 0.47234\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9730 - Jaccard_coef: 0.4507 - val_loss: 0.9793 - val_Jaccard_coef: 0.4686 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9746 - Jaccard_coef: 0.4498\n",
      "Epoch 52: val_Jaccard_coef did not improve from 0.47234\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9746 - Jaccard_coef: 0.4498 - val_loss: 0.9702 - val_Jaccard_coef: 0.4706 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9860 - Jaccard_coef: 0.4458\n",
      "Epoch 53: val_Jaccard_coef did not improve from 0.47234\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9860 - Jaccard_coef: 0.4458 - val_loss: 0.9856 - val_Jaccard_coef: 0.4655 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9686 - Jaccard_coef: 0.4512\n",
      "Epoch 54: val_Jaccard_coef improved from 0.47234 to 0.47590, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9686 - Jaccard_coef: 0.4512 - val_loss: 0.9603 - val_Jaccard_coef: 0.4759 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9632 - Jaccard_coef: 0.4545\n",
      "Epoch 55: val_Jaccard_coef improved from 0.47590 to 0.48537, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9632 - Jaccard_coef: 0.4545 - val_loss: 0.9445 - val_Jaccard_coef: 0.4854 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9617 - Jaccard_coef: 0.4547\n",
      "Epoch 56: val_Jaccard_coef did not improve from 0.48537\n",
      "45/45 [==============================] - 44s 990ms/step - loss: 0.9617 - Jaccard_coef: 0.4547 - val_loss: 0.9499 - val_Jaccard_coef: 0.4800 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9658 - Jaccard_coef: 0.4543\n",
      "Epoch 57: val_Jaccard_coef did not improve from 0.48537\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9658 - Jaccard_coef: 0.4543 - val_loss: 0.9524 - val_Jaccard_coef: 0.4808 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9581 - Jaccard_coef: 0.4556\n",
      "Epoch 58: val_Jaccard_coef improved from 0.48537 to 0.48542, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9581 - Jaccard_coef: 0.4556 - val_loss: 0.9357 - val_Jaccard_coef: 0.4854 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9527 - Jaccard_coef: 0.4585\n",
      "Epoch 59: val_Jaccard_coef did not improve from 0.48542\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9527 - Jaccard_coef: 0.4585 - val_loss: 0.9405 - val_Jaccard_coef: 0.4849 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9361 - Jaccard_coef: 0.4648\n",
      "Epoch 60: val_Jaccard_coef improved from 0.48542 to 0.48885, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9361 - Jaccard_coef: 0.4648 - val_loss: 0.9316 - val_Jaccard_coef: 0.4888 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9259 - Jaccard_coef: 0.4700\n",
      "Epoch 61: val_Jaccard_coef did not improve from 0.48885\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9259 - Jaccard_coef: 0.4700 - val_loss: 0.9284 - val_Jaccard_coef: 0.4882 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9261 - Jaccard_coef: 0.4683\n",
      "Epoch 62: val_Jaccard_coef did not improve from 0.48885\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9261 - Jaccard_coef: 0.4683 - val_loss: 0.9313 - val_Jaccard_coef: 0.4855 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9417 - Jaccard_coef: 0.4618\n",
      "Epoch 63: val_Jaccard_coef did not improve from 0.48885\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9417 - Jaccard_coef: 0.4618 - val_loss: 0.9268 - val_Jaccard_coef: 0.4886 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9371 - Jaccard_coef: 0.4643\n",
      "Epoch 64: val_Jaccard_coef improved from 0.48885 to 0.49347, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9371 - Jaccard_coef: 0.4643 - val_loss: 0.9218 - val_Jaccard_coef: 0.4935 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9200 - Jaccard_coef: 0.4704\n",
      "Epoch 65: val_Jaccard_coef improved from 0.49347 to 0.50004, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9200 - Jaccard_coef: 0.4704 - val_loss: 0.9027 - val_Jaccard_coef: 0.5000 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9066 - Jaccard_coef: 0.4770\n",
      "Epoch 66: val_Jaccard_coef did not improve from 0.50004\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9066 - Jaccard_coef: 0.4770 - val_loss: 0.9141 - val_Jaccard_coef: 0.4966 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9149 - Jaccard_coef: 0.4731\n",
      "Epoch 67: val_Jaccard_coef improved from 0.50004 to 0.50073, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9149 - Jaccard_coef: 0.4731 - val_loss: 0.8983 - val_Jaccard_coef: 0.5007 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9155 - Jaccard_coef: 0.4731\n",
      "Epoch 68: val_Jaccard_coef did not improve from 0.50073\n",
      "45/45 [==============================] - 45s 998ms/step - loss: 0.9155 - Jaccard_coef: 0.4731 - val_loss: 0.9050 - val_Jaccard_coef: 0.5002 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9025 - Jaccard_coef: 0.4788\n",
      "Epoch 69: val_Jaccard_coef did not improve from 0.50073\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9025 - Jaccard_coef: 0.4788 - val_loss: 0.9077 - val_Jaccard_coef: 0.4931 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9119 - Jaccard_coef: 0.4744\n",
      "Epoch 70: val_Jaccard_coef improved from 0.50073 to 0.50211, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9119 - Jaccard_coef: 0.4744 - val_loss: 0.8976 - val_Jaccard_coef: 0.5021 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9021 - Jaccard_coef: 0.4783\n",
      "Epoch 71: val_Jaccard_coef did not improve from 0.50211\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9021 - Jaccard_coef: 0.4783 - val_loss: 0.8960 - val_Jaccard_coef: 0.4984 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9016 - Jaccard_coef: 0.4787\n",
      "Epoch 72: val_Jaccard_coef improved from 0.50211 to 0.50700, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.9016 - Jaccard_coef: 0.4787 - val_loss: 0.8846 - val_Jaccard_coef: 0.5070 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9082 - Jaccard_coef: 0.4759\n",
      "Epoch 73: val_Jaccard_coef did not improve from 0.50700\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.9082 - Jaccard_coef: 0.4759 - val_loss: 0.8974 - val_Jaccard_coef: 0.4975 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8947 - Jaccard_coef: 0.4819\n",
      "Epoch 74: val_Jaccard_coef did not improve from 0.50700\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8947 - Jaccard_coef: 0.4819 - val_loss: 0.8852 - val_Jaccard_coef: 0.5062 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9109 - Jaccard_coef: 0.4731\n",
      "Epoch 75: val_Jaccard_coef improved from 0.50700 to 0.50926, saving model to Trained_Model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 46s 1s/step - loss: 0.9109 - Jaccard_coef: 0.4731 - val_loss: 0.8798 - val_Jaccard_coef: 0.5093 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8867 - Jaccard_coef: 0.4846\n",
      "Epoch 76: val_Jaccard_coef did not improve from 0.50926\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.8867 - Jaccard_coef: 0.4846 - val_loss: 0.8789 - val_Jaccard_coef: 0.5071 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8873 - Jaccard_coef: 0.4837\n",
      "Epoch 77: val_Jaccard_coef did not improve from 0.50926\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.8873 - Jaccard_coef: 0.4837 - val_loss: 0.8751 - val_Jaccard_coef: 0.5089 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8860 - Jaccard_coef: 0.4856\n",
      "Epoch 78: val_Jaccard_coef improved from 0.50926 to 0.51082, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 45s 999ms/step - loss: 0.8860 - Jaccard_coef: 0.4856 - val_loss: 0.8672 - val_Jaccard_coef: 0.5108 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8699 - Jaccard_coef: 0.4903\n",
      "Epoch 79: val_Jaccard_coef improved from 0.51082 to 0.51423, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 936ms/step - loss: 0.8699 - Jaccard_coef: 0.4903 - val_loss: 0.8692 - val_Jaccard_coef: 0.5142 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8921 - Jaccard_coef: 0.4815\n",
      "Epoch 80: val_Jaccard_coef did not improve from 0.51423\n",
      "45/45 [==============================] - 44s 983ms/step - loss: 0.8921 - Jaccard_coef: 0.4815 - val_loss: 0.8650 - val_Jaccard_coef: 0.5129 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8811 - Jaccard_coef: 0.4864\n",
      "Epoch 81: val_Jaccard_coef did not improve from 0.51423\n",
      "45/45 [==============================] - 39s 878ms/step - loss: 0.8811 - Jaccard_coef: 0.4864 - val_loss: 0.8678 - val_Jaccard_coef: 0.5117 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8861 - Jaccard_coef: 0.4832\n",
      "Epoch 82: val_Jaccard_coef did not improve from 0.51423\n",
      "45/45 [==============================] - 39s 873ms/step - loss: 0.8861 - Jaccard_coef: 0.4832 - val_loss: 0.8630 - val_Jaccard_coef: 0.5054 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8760 - Jaccard_coef: 0.4871\n",
      "Epoch 83: val_Jaccard_coef did not improve from 0.51423\n",
      "45/45 [==============================] - 39s 873ms/step - loss: 0.8760 - Jaccard_coef: 0.4871 - val_loss: 0.8624 - val_Jaccard_coef: 0.5140 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8733 - Jaccard_coef: 0.4883\n",
      "Epoch 84: val_Jaccard_coef improved from 0.51423 to 0.51855, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 40s 897ms/step - loss: 0.8733 - Jaccard_coef: 0.4883 - val_loss: 0.8564 - val_Jaccard_coef: 0.5185 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8729 - Jaccard_coef: 0.4895\n",
      "Epoch 85: val_Jaccard_coef improved from 0.51855 to 0.51976, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 40s 887ms/step - loss: 0.8729 - Jaccard_coef: 0.4895 - val_loss: 0.8501 - val_Jaccard_coef: 0.5198 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8682 - Jaccard_coef: 0.4904\n",
      "Epoch 86: val_Jaccard_coef did not improve from 0.51976\n",
      "45/45 [==============================] - 39s 864ms/step - loss: 0.8682 - Jaccard_coef: 0.4904 - val_loss: 0.8510 - val_Jaccard_coef: 0.5191 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8674 - Jaccard_coef: 0.4911\n",
      "Epoch 87: val_Jaccard_coef did not improve from 0.51976\n",
      "45/45 [==============================] - 40s 896ms/step - loss: 0.8674 - Jaccard_coef: 0.4911 - val_loss: 0.8587 - val_Jaccard_coef: 0.5117 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8590 - Jaccard_coef: 0.4947\n",
      "Epoch 88: val_Jaccard_coef did not improve from 0.51976\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.8590 - Jaccard_coef: 0.4947 - val_loss: 0.8404 - val_Jaccard_coef: 0.5190 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8666 - Jaccard_coef: 0.4892\n",
      "Epoch 89: val_Jaccard_coef improved from 0.51976 to 0.52177, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 931ms/step - loss: 0.8666 - Jaccard_coef: 0.4892 - val_loss: 0.8442 - val_Jaccard_coef: 0.5218 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8556 - Jaccard_coef: 0.4958\n",
      "Epoch 90: val_Jaccard_coef did not improve from 0.52177\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 0.8556 - Jaccard_coef: 0.4958 - val_loss: 0.8391 - val_Jaccard_coef: 0.5209 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8607 - Jaccard_coef: 0.4926\n",
      "Epoch 91: val_Jaccard_coef improved from 0.52177 to 0.52288, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 929ms/step - loss: 0.8607 - Jaccard_coef: 0.4926 - val_loss: 0.8365 - val_Jaccard_coef: 0.5229 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8469 - Jaccard_coef: 0.4991\n",
      "Epoch 92: val_Jaccard_coef improved from 0.52288 to 0.52586, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 926ms/step - loss: 0.8469 - Jaccard_coef: 0.4991 - val_loss: 0.8325 - val_Jaccard_coef: 0.5259 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8606 - Jaccard_coef: 0.4929\n",
      "Epoch 93: val_Jaccard_coef did not improve from 0.52586\n",
      "45/45 [==============================] - 41s 908ms/step - loss: 0.8606 - Jaccard_coef: 0.4929 - val_loss: 0.8455 - val_Jaccard_coef: 0.5120 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8529 - Jaccard_coef: 0.4964\n",
      "Epoch 94: val_Jaccard_coef improved from 0.52586 to 0.53063, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 926ms/step - loss: 0.8529 - Jaccard_coef: 0.4964 - val_loss: 0.8226 - val_Jaccard_coef: 0.5306 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8518 - Jaccard_coef: 0.4969\n",
      "Epoch 95: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 914ms/step - loss: 0.8518 - Jaccard_coef: 0.4969 - val_loss: 0.8224 - val_Jaccard_coef: 0.5301 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8388 - Jaccard_coef: 0.5026\n",
      "Epoch 96: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 908ms/step - loss: 0.8388 - Jaccard_coef: 0.5026 - val_loss: 0.8311 - val_Jaccard_coef: 0.5217 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8436 - Jaccard_coef: 0.4996\n",
      "Epoch 97: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.8436 - Jaccard_coef: 0.4996 - val_loss: 0.8302 - val_Jaccard_coef: 0.5209 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8482 - Jaccard_coef: 0.4995\n",
      "Epoch 98: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8482 - Jaccard_coef: 0.4995 - val_loss: 0.8367 - val_Jaccard_coef: 0.5197 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8396 - Jaccard_coef: 0.5020\n",
      "Epoch 99: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 915ms/step - loss: 0.8396 - Jaccard_coef: 0.5020 - val_loss: 0.8201 - val_Jaccard_coef: 0.5255 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8348 - Jaccard_coef: 0.5040\n",
      "Epoch 100: val_Jaccard_coef did not improve from 0.53063\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.8348 - Jaccard_coef: 0.5040 - val_loss: 0.8151 - val_Jaccard_coef: 0.5267 - lr: 0.0010\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - ETA: 0s - loss: 0.8292 - Jaccard_coef: 0.5061\n",
      "Epoch 101: val_Jaccard_coef improved from 0.53063 to 0.53974, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 41s 924ms/step - loss: 0.8292 - Jaccard_coef: 0.5061 - val_loss: 0.8074 - val_Jaccard_coef: 0.5397 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8512 - Jaccard_coef: 0.4968\n",
      "Epoch 102: val_Jaccard_coef did not improve from 0.53974\n",
      "45/45 [==============================] - 40s 902ms/step - loss: 0.8512 - Jaccard_coef: 0.4968 - val_loss: 0.8252 - val_Jaccard_coef: 0.5212 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8428 - Jaccard_coef: 0.5005\n",
      "Epoch 103: val_Jaccard_coef did not improve from 0.53974\n",
      "45/45 [==============================] - 41s 903ms/step - loss: 0.8428 - Jaccard_coef: 0.5005 - val_loss: 0.8137 - val_Jaccard_coef: 0.5312 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8196 - Jaccard_coef: 0.5096\n",
      "Epoch 104: val_Jaccard_coef improved from 0.53974 to 0.54081, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 941ms/step - loss: 0.8196 - Jaccard_coef: 0.5096 - val_loss: 0.7975 - val_Jaccard_coef: 0.5408 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8305 - Jaccard_coef: 0.5051\n",
      "Epoch 105: val_Jaccard_coef did not improve from 0.54081\n",
      "45/45 [==============================] - 41s 915ms/step - loss: 0.8305 - Jaccard_coef: 0.5051 - val_loss: 0.8122 - val_Jaccard_coef: 0.5307 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8439 - Jaccard_coef: 0.4997\n",
      "Epoch 106: val_Jaccard_coef did not improve from 0.54081\n",
      "45/45 [==============================] - 41s 905ms/step - loss: 0.8439 - Jaccard_coef: 0.4997 - val_loss: 0.8088 - val_Jaccard_coef: 0.5313 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8256 - Jaccard_coef: 0.5058\n",
      "Epoch 107: val_Jaccard_coef did not improve from 0.54081\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8256 - Jaccard_coef: 0.5058 - val_loss: 0.8046 - val_Jaccard_coef: 0.5329 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8277 - Jaccard_coef: 0.5062\n",
      "Epoch 108: val_Jaccard_coef did not improve from 0.54081\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.8277 - Jaccard_coef: 0.5062 - val_loss: 0.7956 - val_Jaccard_coef: 0.5403 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8209 - Jaccard_coef: 0.5084\n",
      "Epoch 109: val_Jaccard_coef did not improve from 0.54081\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.8209 - Jaccard_coef: 0.5084 - val_loss: 0.8141 - val_Jaccard_coef: 0.5219 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8237 - Jaccard_coef: 0.5079\n",
      "Epoch 110: val_Jaccard_coef improved from 0.54081 to 0.54568, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 930ms/step - loss: 0.8237 - Jaccard_coef: 0.5079 - val_loss: 0.7830 - val_Jaccard_coef: 0.5457 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8210 - Jaccard_coef: 0.5084\n",
      "Epoch 111: val_Jaccard_coef did not improve from 0.54568\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 0.8210 - Jaccard_coef: 0.5084 - val_loss: 0.8057 - val_Jaccard_coef: 0.5302 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8378 - Jaccard_coef: 0.5012\n",
      "Epoch 112: val_Jaccard_coef did not improve from 0.54568\n",
      "45/45 [==============================] - 41s 905ms/step - loss: 0.8378 - Jaccard_coef: 0.5012 - val_loss: 0.8138 - val_Jaccard_coef: 0.5299 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8139 - Jaccard_coef: 0.5110\n",
      "Epoch 113: val_Jaccard_coef improved from 0.54568 to 0.54618, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 42s 926ms/step - loss: 0.8139 - Jaccard_coef: 0.5110 - val_loss: 0.7805 - val_Jaccard_coef: 0.5462 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8179 - Jaccard_coef: 0.5115\n",
      "Epoch 114: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8179 - Jaccard_coef: 0.5115 - val_loss: 0.7974 - val_Jaccard_coef: 0.5371 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8100 - Jaccard_coef: 0.5127\n",
      "Epoch 115: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 911ms/step - loss: 0.8100 - Jaccard_coef: 0.5127 - val_loss: 0.8046 - val_Jaccard_coef: 0.5270 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8251 - Jaccard_coef: 0.5067\n",
      "Epoch 116: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.8251 - Jaccard_coef: 0.5067 - val_loss: 0.7896 - val_Jaccard_coef: 0.5382 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8248 - Jaccard_coef: 0.5059\n",
      "Epoch 117: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 40s 899ms/step - loss: 0.8248 - Jaccard_coef: 0.5059 - val_loss: 0.7930 - val_Jaccard_coef: 0.5341 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8063 - Jaccard_coef: 0.5154\n",
      "Epoch 118: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 912ms/step - loss: 0.8063 - Jaccard_coef: 0.5154 - val_loss: 0.7850 - val_Jaccard_coef: 0.5405 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8076 - Jaccard_coef: 0.5145\n",
      "Epoch 119: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.8076 - Jaccard_coef: 0.5145 - val_loss: 0.7815 - val_Jaccard_coef: 0.5413 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8144 - Jaccard_coef: 0.5114\n",
      "Epoch 120: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 40s 901ms/step - loss: 0.8144 - Jaccard_coef: 0.5114 - val_loss: 0.7958 - val_Jaccard_coef: 0.5333 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8155 - Jaccard_coef: 0.5097\n",
      "Epoch 121: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "45/45 [==============================] - 41s 912ms/step - loss: 0.8155 - Jaccard_coef: 0.5097 - val_loss: 0.7854 - val_Jaccard_coef: 0.5392 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8107 - Jaccard_coef: 0.5121\n",
      "Epoch 122: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 917ms/step - loss: 0.8107 - Jaccard_coef: 0.5121 - val_loss: 0.7855 - val_Jaccard_coef: 0.5393 - lr: 5.0000e-04\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8011 - Jaccard_coef: 0.5165\n",
      "Epoch 123: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 911ms/step - loss: 0.8011 - Jaccard_coef: 0.5165 - val_loss: 0.7839 - val_Jaccard_coef: 0.5375 - lr: 5.0000e-04\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8256 - Jaccard_coef: 0.5058\n",
      "Epoch 124: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 917ms/step - loss: 0.8256 - Jaccard_coef: 0.5058 - val_loss: 0.7915 - val_Jaccard_coef: 0.5397 - lr: 5.0000e-04\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8147 - Jaccard_coef: 0.5103\n",
      "Epoch 125: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 903ms/step - loss: 0.8147 - Jaccard_coef: 0.5103 - val_loss: 0.7880 - val_Jaccard_coef: 0.5351 - lr: 5.0000e-04\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7991 - Jaccard_coef: 0.5174\n",
      "Epoch 126: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 911ms/step - loss: 0.7991 - Jaccard_coef: 0.5174 - val_loss: 0.7876 - val_Jaccard_coef: 0.5380 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8040 - Jaccard_coef: 0.5148\n",
      "Epoch 127: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 919ms/step - loss: 0.8040 - Jaccard_coef: 0.5148 - val_loss: 0.7787 - val_Jaccard_coef: 0.5441 - lr: 5.0000e-04\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8048 - Jaccard_coef: 0.5142\n",
      "Epoch 128: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 40s 902ms/step - loss: 0.8048 - Jaccard_coef: 0.5142 - val_loss: 0.7832 - val_Jaccard_coef: 0.5416 - lr: 5.0000e-04\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7936 - Jaccard_coef: 0.5207\n",
      "Epoch 129: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "45/45 [==============================] - 41s 914ms/step - loss: 0.7936 - Jaccard_coef: 0.5207 - val_loss: 0.7825 - val_Jaccard_coef: 0.5390 - lr: 5.0000e-04\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8081 - Jaccard_coef: 0.5136\n",
      "Epoch 130: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 906ms/step - loss: 0.8081 - Jaccard_coef: 0.5136 - val_loss: 0.7941 - val_Jaccard_coef: 0.5246 - lr: 2.5000e-04\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8013 - Jaccard_coef: 0.5165\n",
      "Epoch 131: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 918ms/step - loss: 0.8013 - Jaccard_coef: 0.5165 - val_loss: 0.7838 - val_Jaccard_coef: 0.5426 - lr: 2.5000e-04\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8103 - Jaccard_coef: 0.5112\n",
      "Epoch 132: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.8103 - Jaccard_coef: 0.5112 - val_loss: 0.7787 - val_Jaccard_coef: 0.5415 - lr: 2.5000e-04\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8260 - Jaccard_coef: 0.5072\n",
      "Epoch 133: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 906ms/step - loss: 0.8260 - Jaccard_coef: 0.5072 - val_loss: 0.7876 - val_Jaccard_coef: 0.5369 - lr: 2.5000e-04\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8050 - Jaccard_coef: 0.5142\n",
      "Epoch 134: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 914ms/step - loss: 0.8050 - Jaccard_coef: 0.5142 - val_loss: 0.7797 - val_Jaccard_coef: 0.5403 - lr: 2.5000e-04\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7804 - Jaccard_coef: 0.5247\n",
      "Epoch 135: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 0.7804 - Jaccard_coef: 0.5247 - val_loss: 0.7803 - val_Jaccard_coef: 0.5416 - lr: 2.5000e-04\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8206 - Jaccard_coef: 0.5081\n",
      "Epoch 136: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 903ms/step - loss: 0.8206 - Jaccard_coef: 0.5081 - val_loss: 0.7850 - val_Jaccard_coef: 0.5373 - lr: 2.5000e-04\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8139 - Jaccard_coef: 0.5096\n",
      "Epoch 137: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8139 - Jaccard_coef: 0.5096 - val_loss: 0.7843 - val_Jaccard_coef: 0.5401 - lr: 2.5000e-04\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7924 - Jaccard_coef: 0.5199\n",
      "Epoch 138: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 915ms/step - loss: 0.7924 - Jaccard_coef: 0.5199 - val_loss: 0.7826 - val_Jaccard_coef: 0.5396 - lr: 1.2500e-04\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8271 - Jaccard_coef: 0.5038\n",
      "Epoch 139: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8271 - Jaccard_coef: 0.5038 - val_loss: 0.7883 - val_Jaccard_coef: 0.5392 - lr: 1.2500e-04\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8020 - Jaccard_coef: 0.5159\n",
      "Epoch 140: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 907ms/step - loss: 0.8020 - Jaccard_coef: 0.5159 - val_loss: 0.7823 - val_Jaccard_coef: 0.5417 - lr: 1.2500e-04\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8053 - Jaccard_coef: 0.5151\n",
      "Epoch 141: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8053 - Jaccard_coef: 0.5151 - val_loss: 0.7833 - val_Jaccard_coef: 0.5407 - lr: 1.2500e-04\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8126 - Jaccard_coef: 0.5117\n",
      "Epoch 142: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.8126 - Jaccard_coef: 0.5117 - val_loss: 0.7781 - val_Jaccard_coef: 0.5449 - lr: 1.2500e-04\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7968 - Jaccard_coef: 0.5169\n",
      "Epoch 143: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 919ms/step - loss: 0.7968 - Jaccard_coef: 0.5169 - val_loss: 0.7969 - val_Jaccard_coef: 0.5293 - lr: 1.2500e-04\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8041 - Jaccard_coef: 0.5152\n",
      "Epoch 144: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 914ms/step - loss: 0.8041 - Jaccard_coef: 0.5152 - val_loss: 0.7873 - val_Jaccard_coef: 0.5396 - lr: 1.2500e-04\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7935 - Jaccard_coef: 0.5202\n",
      "Epoch 145: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 0.7935 - Jaccard_coef: 0.5202 - val_loss: 0.7751 - val_Jaccard_coef: 0.5432 - lr: 1.2500e-04\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8049 - Jaccard_coef: 0.5157\n",
      "Epoch 146: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 909ms/step - loss: 0.8049 - Jaccard_coef: 0.5157 - val_loss: 0.7750 - val_Jaccard_coef: 0.5448 - lr: 6.2500e-05\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7894 - Jaccard_coef: 0.5208\n",
      "Epoch 147: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 42s 929ms/step - loss: 0.7894 - Jaccard_coef: 0.5208 - val_loss: 0.7895 - val_Jaccard_coef: 0.5358 - lr: 6.2500e-05\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7994 - Jaccard_coef: 0.5169\n",
      "Epoch 148: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 961ms/step - loss: 0.7994 - Jaccard_coef: 0.5169 - val_loss: 0.7815 - val_Jaccard_coef: 0.5407 - lr: 6.2500e-05\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8043 - Jaccard_coef: 0.5165\n",
      "Epoch 149: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.8043 - Jaccard_coef: 0.5165 - val_loss: 0.7801 - val_Jaccard_coef: 0.5416 - lr: 6.2500e-05\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7952 - Jaccard_coef: 0.5183\n",
      "Epoch 150: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.7952 - Jaccard_coef: 0.5183 - val_loss: 0.7875 - val_Jaccard_coef: 0.5331 - lr: 6.2500e-05\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8158 - Jaccard_coef: 0.5095\n",
      "Epoch 151: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8158 - Jaccard_coef: 0.5095 - val_loss: 0.7818 - val_Jaccard_coef: 0.5408 - lr: 6.2500e-05\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - ETA: 0s - loss: 0.8004 - Jaccard_coef: 0.5162\n",
      "Epoch 152: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.8004 - Jaccard_coef: 0.5162 - val_loss: 0.7816 - val_Jaccard_coef: 0.5404 - lr: 6.2500e-05\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8037 - Jaccard_coef: 0.5161\n",
      "Epoch 153: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8037 - Jaccard_coef: 0.5161 - val_loss: 0.7874 - val_Jaccard_coef: 0.5345 - lr: 6.2500e-05\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7992 - Jaccard_coef: 0.5172\n",
      "Epoch 154: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.7992 - Jaccard_coef: 0.5172 - val_loss: 0.7847 - val_Jaccard_coef: 0.5423 - lr: 3.1250e-05\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8002 - Jaccard_coef: 0.5161\n",
      "Epoch 155: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8002 - Jaccard_coef: 0.5161 - val_loss: 0.7803 - val_Jaccard_coef: 0.5387 - lr: 3.1250e-05\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7927 - Jaccard_coef: 0.5195\n",
      "Epoch 156: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 44s 986ms/step - loss: 0.7927 - Jaccard_coef: 0.5195 - val_loss: 0.7864 - val_Jaccard_coef: 0.5365 - lr: 3.1250e-05\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7999 - Jaccard_coef: 0.5178\n",
      "Epoch 157: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 963ms/step - loss: 0.7999 - Jaccard_coef: 0.5178 - val_loss: 0.7815 - val_Jaccard_coef: 0.5383 - lr: 3.1250e-05\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8017 - Jaccard_coef: 0.5163\n",
      "Epoch 158: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 965ms/step - loss: 0.8017 - Jaccard_coef: 0.5163 - val_loss: 0.7816 - val_Jaccard_coef: 0.5407 - lr: 3.1250e-05\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8112 - Jaccard_coef: 0.5127\n",
      "Epoch 159: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 959ms/step - loss: 0.8112 - Jaccard_coef: 0.5127 - val_loss: 0.7868 - val_Jaccard_coef: 0.5371 - lr: 3.1250e-05\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7961 - Jaccard_coef: 0.5167\n",
      "Epoch 160: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 969ms/step - loss: 0.7961 - Jaccard_coef: 0.5167 - val_loss: 0.7826 - val_Jaccard_coef: 0.5392 - lr: 3.1250e-05\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8310 - Jaccard_coef: 0.5026\n",
      "Epoch 161: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "45/45 [==============================] - 43s 955ms/step - loss: 0.8310 - Jaccard_coef: 0.5026 - val_loss: 0.7828 - val_Jaccard_coef: 0.5433 - lr: 3.1250e-05\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7953 - Jaccard_coef: 0.5185\n",
      "Epoch 162: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 44s 971ms/step - loss: 0.7953 - Jaccard_coef: 0.5185 - val_loss: 0.7865 - val_Jaccard_coef: 0.5349 - lr: 1.5625e-05\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8107 - Jaccard_coef: 0.5124\n",
      "Epoch 163: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 952ms/step - loss: 0.8107 - Jaccard_coef: 0.5124 - val_loss: 0.7836 - val_Jaccard_coef: 0.5413 - lr: 1.5625e-05\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7940 - Jaccard_coef: 0.5196\n",
      "Epoch 164: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 913ms/step - loss: 0.7940 - Jaccard_coef: 0.5196 - val_loss: 0.7886 - val_Jaccard_coef: 0.5361 - lr: 1.5625e-05\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7991 - Jaccard_coef: 0.5176\n",
      "Epoch 165: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 41s 904ms/step - loss: 0.7991 - Jaccard_coef: 0.5176 - val_loss: 0.7804 - val_Jaccard_coef: 0.5408 - lr: 1.5625e-05\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7936 - Jaccard_coef: 0.5188\n",
      "Epoch 166: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 956ms/step - loss: 0.7936 - Jaccard_coef: 0.5188 - val_loss: 0.7807 - val_Jaccard_coef: 0.5409 - lr: 1.5625e-05\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8028 - Jaccard_coef: 0.5160\n",
      "Epoch 167: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 44s 969ms/step - loss: 0.8028 - Jaccard_coef: 0.5160 - val_loss: 0.7800 - val_Jaccard_coef: 0.5416 - lr: 1.5625e-05\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8041 - Jaccard_coef: 0.5150\n",
      "Epoch 168: val_Jaccard_coef did not improve from 0.54618\n",
      "45/45 [==============================] - 43s 968ms/step - loss: 0.8041 - Jaccard_coef: 0.5150 - val_loss: 0.7783 - val_Jaccard_coef: 0.5441 - lr: 1.5625e-05\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8080 - Jaccard_coef: 0.5126\n",
      "Epoch 169: val_Jaccard_coef did not improve from 0.54618\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "45/45 [==============================] - 43s 969ms/step - loss: 0.8080 - Jaccard_coef: 0.5126 - val_loss: 0.7882 - val_Jaccard_coef: 0.5365 - lr: 1.5625e-05\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7940 - Jaccard_coef: 0.5202\n",
      "Epoch 170: val_Jaccard_coef improved from 0.54618 to 0.55028, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 44s 980ms/step - loss: 0.7940 - Jaccard_coef: 0.5202 - val_loss: 0.7715 - val_Jaccard_coef: 0.5503 - lr: 7.8125e-06\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8059 - Jaccard_coef: 0.5138\n",
      "Epoch 171: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 43s 955ms/step - loss: 0.8059 - Jaccard_coef: 0.5138 - val_loss: 0.7815 - val_Jaccard_coef: 0.5359 - lr: 7.8125e-06\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8037 - Jaccard_coef: 0.5146\n",
      "Epoch 172: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 43s 950ms/step - loss: 0.8037 - Jaccard_coef: 0.5146 - val_loss: 0.7870 - val_Jaccard_coef: 0.5390 - lr: 7.8125e-06\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8155 - Jaccard_coef: 0.5106\n",
      "Epoch 173: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.8155 - Jaccard_coef: 0.5106 - val_loss: 0.7846 - val_Jaccard_coef: 0.5432 - lr: 7.8125e-06\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8181 - Jaccard_coef: 0.5089\n",
      "Epoch 174: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 42s 938ms/step - loss: 0.8181 - Jaccard_coef: 0.5089 - val_loss: 0.7805 - val_Jaccard_coef: 0.5418 - lr: 7.8125e-06\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7901 - Jaccard_coef: 0.5209\n",
      "Epoch 175: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 41s 918ms/step - loss: 0.7901 - Jaccard_coef: 0.5209 - val_loss: 0.7862 - val_Jaccard_coef: 0.5358 - lr: 7.8125e-06\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7861 - Jaccard_coef: 0.5222\n",
      "Epoch 176: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 41s 903ms/step - loss: 0.7861 - Jaccard_coef: 0.5222 - val_loss: 0.7903 - val_Jaccard_coef: 0.5420 - lr: 7.8125e-06\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - ETA: 0s - loss: 0.7984 - Jaccard_coef: 0.5177\n",
      "Epoch 177: val_Jaccard_coef did not improve from 0.55028\n",
      "45/45 [==============================] - 41s 917ms/step - loss: 0.7984 - Jaccard_coef: 0.5177 - val_loss: 0.7899 - val_Jaccard_coef: 0.5334 - lr: 7.8125e-06\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7957 - Jaccard_coef: 0.5179\n",
      "Epoch 178: val_Jaccard_coef did not improve from 0.55028\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "45/45 [==============================] - 43s 959ms/step - loss: 0.7957 - Jaccard_coef: 0.5179 - val_loss: 0.7844 - val_Jaccard_coef: 0.5342 - lr: 7.8125e-06\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7939 - Jaccard_coef: 0.5184\n",
      "Epoch 179: val_Jaccard_coef improved from 0.55028 to 0.55067, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 43s 948ms/step - loss: 0.7939 - Jaccard_coef: 0.5184 - val_loss: 0.7710 - val_Jaccard_coef: 0.5507 - lr: 3.9063e-06\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8158 - Jaccard_coef: 0.5101\n",
      "Epoch 180: val_Jaccard_coef did not improve from 0.55067\n",
      "45/45 [==============================] - 41s 911ms/step - loss: 0.8158 - Jaccard_coef: 0.5101 - val_loss: 0.7891 - val_Jaccard_coef: 0.5358 - lr: 3.9063e-06\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7929 - Jaccard_coef: 0.5203\n",
      "Epoch 181: val_Jaccard_coef did not improve from 0.55067\n",
      "45/45 [==============================] - 42s 933ms/step - loss: 0.7929 - Jaccard_coef: 0.5203 - val_loss: 0.7818 - val_Jaccard_coef: 0.5403 - lr: 3.9063e-06\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8010 - Jaccard_coef: 0.5154\n",
      "Epoch 182: val_Jaccard_coef did not improve from 0.55067\n",
      "45/45 [==============================] - 43s 955ms/step - loss: 0.8010 - Jaccard_coef: 0.5154 - val_loss: 0.7885 - val_Jaccard_coef: 0.5356 - lr: 3.9063e-06\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7986 - Jaccard_coef: 0.5167\n",
      "Epoch 183: val_Jaccard_coef did not improve from 0.55067\n",
      "45/45 [==============================] - 43s 963ms/step - loss: 0.7986 - Jaccard_coef: 0.5167 - val_loss: 0.7839 - val_Jaccard_coef: 0.5422 - lr: 3.9063e-06\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8067 - Jaccard_coef: 0.5135\n",
      "Epoch 184: val_Jaccard_coef did not improve from 0.55067\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8067 - Jaccard_coef: 0.5135 - val_loss: 0.7924 - val_Jaccard_coef: 0.5351 - lr: 3.9063e-06\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8062 - Jaccard_coef: 0.5153\n",
      "Epoch 185: val_Jaccard_coef improved from 0.55067 to 0.55257, saving model to Trained_Model.hdf5\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.8062 - Jaccard_coef: 0.5153 - val_loss: 0.7646 - val_Jaccard_coef: 0.5526 - lr: 3.9063e-06\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8019 - Jaccard_coef: 0.5147\n",
      "Epoch 186: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8019 - Jaccard_coef: 0.5147 - val_loss: 0.7925 - val_Jaccard_coef: 0.5322 - lr: 3.9063e-06\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7900 - Jaccard_coef: 0.5218\n",
      "Epoch 187: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.7900 - Jaccard_coef: 0.5218 - val_loss: 0.7884 - val_Jaccard_coef: 0.5408 - lr: 3.9063e-06\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7926 - Jaccard_coef: 0.5208\n",
      "Epoch 188: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 996ms/step - loss: 0.7926 - Jaccard_coef: 0.5208 - val_loss: 0.7859 - val_Jaccard_coef: 0.5357 - lr: 3.9063e-06\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8079 - Jaccard_coef: 0.5140\n",
      "Epoch 189: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 42s 931ms/step - loss: 0.8079 - Jaccard_coef: 0.5140 - val_loss: 0.7733 - val_Jaccard_coef: 0.5506 - lr: 3.9063e-06\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8015 - Jaccard_coef: 0.5155\n",
      "Epoch 190: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 43s 962ms/step - loss: 0.8015 - Jaccard_coef: 0.5155 - val_loss: 0.7826 - val_Jaccard_coef: 0.5368 - lr: 3.9063e-06\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8005 - Jaccard_coef: 0.5158\n",
      "Epoch 191: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 998ms/step - loss: 0.8005 - Jaccard_coef: 0.5158 - val_loss: 0.7743 - val_Jaccard_coef: 0.5475 - lr: 3.9063e-06\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7879 - Jaccard_coef: 0.5214\n",
      "Epoch 192: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 44s 975ms/step - loss: 0.7879 - Jaccard_coef: 0.5214 - val_loss: 0.7877 - val_Jaccard_coef: 0.5363 - lr: 3.9063e-06\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8096 - Jaccard_coef: 0.5129\n",
      "Epoch 193: val_Jaccard_coef did not improve from 0.55257\n",
      "\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8096 - Jaccard_coef: 0.5129 - val_loss: 0.7755 - val_Jaccard_coef: 0.5456 - lr: 3.9063e-06\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7886 - Jaccard_coef: 0.5223\n",
      "Epoch 194: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 997ms/step - loss: 0.7886 - Jaccard_coef: 0.5223 - val_loss: 0.7728 - val_Jaccard_coef: 0.5447 - lr: 1.9531e-06\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7957 - Jaccard_coef: 0.5180\n",
      "Epoch 195: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 994ms/step - loss: 0.7957 - Jaccard_coef: 0.5180 - val_loss: 0.7915 - val_Jaccard_coef: 0.5359 - lr: 1.9531e-06\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7980 - Jaccard_coef: 0.5178\n",
      "Epoch 196: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 44s 985ms/step - loss: 0.7980 - Jaccard_coef: 0.5178 - val_loss: 0.7846 - val_Jaccard_coef: 0.5380 - lr: 1.9531e-06\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7880 - Jaccard_coef: 0.5226\n",
      "Epoch 197: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.7880 - Jaccard_coef: 0.5226 - val_loss: 0.7834 - val_Jaccard_coef: 0.5403 - lr: 1.9531e-06\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7960 - Jaccard_coef: 0.5177\n",
      "Epoch 198: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 43s 950ms/step - loss: 0.7960 - Jaccard_coef: 0.5177 - val_loss: 0.7778 - val_Jaccard_coef: 0.5443 - lr: 1.9531e-06\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8004 - Jaccard_coef: 0.5160\n",
      "Epoch 199: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 44s 989ms/step - loss: 0.8004 - Jaccard_coef: 0.5160 - val_loss: 0.7833 - val_Jaccard_coef: 0.5424 - lr: 1.9531e-06\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8092 - Jaccard_coef: 0.5114\n",
      "Epoch 200: val_Jaccard_coef did not improve from 0.55257\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.8092 - Jaccard_coef: 0.5114 - val_loss: 0.7802 - val_Jaccard_coef: 0.5421 - lr: 1.9531e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model.fit_generator(TrainGen, \n",
    "                             steps_per_epoch=45, \n",
    "                             epochs=200, \n",
    "                             verbose=1, \n",
    "                             validation_data= TestGen, \n",
    "                             validation_steps=30, \n",
    "                             callbacks=[model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFaGVLA7PkxT"
   },
   "source": [
    "## Plotting the train and validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7ypdAa2PkxT"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Plot training & validation accuracy values\n",
    "plt.plot(history.history['Jaccard_coef'])\n",
    "plt.plot(history.history['val_Jaccard_coef'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('dice_coef')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    " # Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YBLPAmxPkxW"
   },
   "outputs": [],
   "source": [
    "def Performance_Metrics(y_true, y_pred):\n",
    "    ''' \n",
    "    The Dice similarity coefficient (DSC) is a statistical validation metric for the image\n",
    "    segmentations and determines the spatial overlap accuracy of the segmentation. It also \n",
    "    same as F1-score. \n",
    "    \n",
    "    The Intersection over Union (IoU) also referred to as the Jaccard index (JI), is essentially\n",
    "    a method to quantify the percent overlap between the GT mask and prediction output.\n",
    "    This metric is closely related to the DSC. The IoU metric measures the number of pixels\n",
    "    common between the target and prediction masks divided by the total number of pixels present\n",
    "    across both masks.\n",
    "    \n",
    "    Mathematically, they are related as follows:\n",
    "    IoU = DSC/(2-DSC) or DSC = 2*IoU/(IoU+1)\n",
    "    \n",
    "    The Sensitivity also called the true positive rate, the recall, or probability of detection that \n",
    "    measures the proportion of actual positives that are correctly identified as such class \n",
    "    (e.g., the percentage of sick people who are correctly identified as having the condition).\n",
    "    \n",
    "    The Specificity also called the true negative rate measures the proportion of actual negatives that\n",
    "    are correctly identified as such class. (e.g., the percentage of healthy people who are correctly\n",
    "    identified as not having the condition).\n",
    "    \n",
    "  \n",
    "    Input Arguments: \n",
    "        y_true: True Labels of the 2D images so called ground truth (GT).\n",
    "        y_pred: Predicted Labels of the 2D images so called Predicted/ segmented Mask.\n",
    "        \n",
    "    Output Arguments: \n",
    "        dsc: The DSC between y_true and y_pred\n",
    "        iou: The IoU between y_true and y_pred\n",
    "        accuracy: The accuracy of pixels clasification.\n",
    "        sensitivity: The accuracy of foreground pixels clasification.\n",
    "        specificity: The accuracy of background pixels clasification.\n",
    "        balancedAccuracy: The balanced accuracy of both foreground & background pixels clasification.\n",
    "        \n",
    "    Author: Md. Kamrul Hasan, \n",
    "            Erasmus Scholar on Medical Imaging and Application (MAIA)\n",
    "            E-mail: kamruleeekuet@gmail.com\n",
    "\n",
    "    '''\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch!! y_true and y_pred must have the same shape.\")\n",
    "\n",
    "    y_true_f = (y_true/y_true.max()).flatten()\n",
    "    y_pred_f = (y_pred/y_pred.max()).flatten()\n",
    "    \n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    dsc = (2. * intersection ) / (np.sum(y_true_f) + np.sum(y_pred_f))\n",
    "    iou = (intersection) / (np.sum(y_true_f) + np.sum(y_pred_f)-intersection)\n",
    "    \n",
    "    y_true = np.asarray(y_true).astype(np.bool)\n",
    "    y_pred = np.asarray(y_pred).astype(np.bool)\n",
    "\n",
    "        \n",
    "    y_true=y_true.flatten()\n",
    "    y_pred=y_pred.flatten()\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "    sensitivity = (tp)/(fn+tp)\n",
    "    specificity = (tn)/(tn+fp)\n",
    "    balancedAccuracy = (sensitivity+specificity)/2\n",
    "\n",
    "    \n",
    "    return dsc, iou, accuracy, sensitivity, specificity, balancedAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rBVOlJRSzOP"
   },
   "source": [
    "## Mask Prediction from DSNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGZmY59lPkxc",
    "outputId": "ebb62be7-762b-465b-cced-858cdd608bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Predict the masks using the proposed DSNet and those masks are processed using \n",
    "# the “DSNet_Performance.m” to estimate the performance metrics. The ROC curves \n",
    "# are plotted using the “ROC_DSNet.ipynb” \n",
    "\n",
    "testImagePath =CurrentDirectory+\"/isic-challenge-2016_split/Test_Input\"\n",
    "imagePath = glob.glob(testImagePath+\"*.jpg\")\n",
    "imagePath.sort()\n",
    "print(len(imagePath))\n",
    "\n",
    "GTPath= CurrentDirectory+\"/isic-challenge-2016_split/Test_Mask\"\n",
    "GT = glob.glob(GTPath+\"*.png\")\n",
    "GT.sort()\n",
    "print(len(GT))\n",
    "\n",
    "\n",
    "MaskSavePath= CurrentDirectory+'/isic-challenge-2016_split/Output_Mask'\n",
    "\n",
    "model = DSNet(2, height, width)\n",
    "\n",
    "model.load_weights(\"Trained_Model.hdf5\")\n",
    "\n",
    "for imageName,gt in zip(imagePath,GT):\n",
    "    img = cv2.imread(imageName,-1)\n",
    "    imgcopy=img.copy()\n",
    "    filename, file_extension = os.path.splitext(imageName) \n",
    "    \n",
    "    img=cv2.resize(img,(width,height))\n",
    "    img=img.astype(np.float32)\n",
    "    img = img/img.max()\n",
    "    img=np.expand_dims(img, axis=0)\n",
    "    Prediction=model.predict(img,verbose=0)\n",
    "    \n",
    "    mask=Prediction.reshape(height,width)\n",
    "    mask=(255*(mask/mask.max())).astype('uint8')\n",
    "    cv2.imwrite(MaskSavePath+filename[-12:]+'.png',mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NvNMsulPkxf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
